{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Turkic_nlp.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbQokbwCdXde"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYeq7NlYvO9Q"
      },
      "source": [
        "import re\r\n",
        "import os\r\n",
        "import xlrd, xlwt\r\n",
        "import pandas as pd\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-virlFRt_ge"
      },
      "source": [
        "endings_wb = pd.read_excel(\"/content/wb.xlsx\")\r\n",
        "endings_wb[\"affix\"] = endings_wb[\"Endings\"].str.replace('-','')\r\n",
        "endings_wb[\"segm\"] = endings_wb[\"Endings\"].str.replace('-','@@')\r\n",
        "endingsnew = list()\r\n",
        "endingsnew_probel = list()\r\n",
        "\r\n",
        "\r\n",
        "for i, row in endings_wb.iterrows():\r\n",
        "    endingsnew.append(row['affix'])\r\n",
        "    endingsnew_probel.append(row['segm'])\r\n",
        "\r\n",
        "\r\n",
        "res = {endingsnew[i]: endingsnew_probel[i] for i in range(len(endingsnew))}\r\n",
        "\r\n",
        "\r\n",
        "endings_wb.to_excel(\"/content/wb.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35bD8TfIkNRJ",
        "outputId": "017d378f-d2a2-43e8-d187-d18ad503313c"
      },
      "source": [
        "def sorting_affixes(file_name):\r\n",
        "    affixes_sh=affixes_wb= pd.read_excel(affixes_file_name, usecols=['affix'])\r\n",
        "  \r\n",
        "    affixes = list()\r\n",
        "\r\n",
        "    for i, row in affixes_sh.iterrows():\r\n",
        "        affixes.append(row['affix'])\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "    sorted_affixes = sorted(affixes, key=len, reverse=True)\r\n",
        "\r\n",
        "    return sorted_affixes\r\n",
        "\r\n",
        "def stem(word, affixes):\r\n",
        "    word_len = len(word)\r\n",
        "    min_len_of_word = 2\r\n",
        "    stems = []\r\n",
        "\r\n",
        "    if word_len > min_len_of_word:\r\n",
        "            n = word_len - min_len_of_word\r\n",
        "            for i in range(n+1, 0, -1):\r\n",
        "                word_affix = word[word_len - (i-1):]\r\n",
        "                stem = word[:word_len-len(word_affix)]\r\n",
        "                for affix in affixes:\r\n",
        "                    if affix == word_affix:\r\n",
        "                        stems.append(stem)\r\n",
        "                    elif affix == '' or word_affix == '':\r\n",
        "            \r\n",
        "                        stems.append(word)\r\n",
        "\r\n",
        "    else:\r\n",
        "        stems.append(word)\r\n",
        "                        \r\n",
        "    return stems[0]\r\n",
        "    \r\n",
        "\r\n",
        "def stemming(file_name, affixes):\r\n",
        "    text_file = open(file_name, 'r', encoding=\"utf-8\")\r\n",
        "    text_file = text_file.read()\r\n",
        "    text = re.sub(r\"[,@\\?\\.$%_]\", \"\", text_file).split()\r\n",
        "    res_text = []\r\n",
        "    for word in text:\r\n",
        "        if word not in res_text:\r\n",
        "            res_text.append(word)\r\n",
        "\r\n",
        "    stem_text = {}\r\n",
        "    for word in res_text:\r\n",
        "        stemm = stem(word, affixes)\r\n",
        "        stem_text.update({word: stemm})\r\n",
        "\r\n",
        "    for i in stem_text.keys():\r\n",
        "        word = str(i)\r\n",
        "        stemm = str(stem_text[i])\r\n",
        "\r\n",
        "        if word in text_file:\r\n",
        "            text_file = re.sub((rf\"\\b{word}\\b\"), stemm, text_file)\r\n",
        "       \r\n",
        "    return text_file\r\n",
        "    \r\n",
        "\r\n",
        "affixes_file_name = input(\"Name of the affix file: \") #\"affixes.xls\"\r\n",
        "affixes = sorting_affixes(affixes_file_name)\r\n",
        "#print(affixes)\r\n",
        "\r\n",
        "text_file_name = input(\"Name of the text file: \") #\"text.txt\"\r\n",
        "text = stemming(text_file_name, affixes)\r\n",
        "\r\n",
        "with open(\"stems.txt\", \"a\", encoding=\"utf-8\") as f1:\r\n",
        "    for d in re.sub(r\"[,@\\?\\.$%_]\", \"\", text).split():\r\n",
        "        f1.write(d.lower() + os.linesep)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#output_file_name = input(\"Name of the output file (result): \") #\"results.txt\"\r\n",
        "#output_file = open(output_file_name, 'w', encoding=\"utf-8\")\r\n",
        "#output_file.write(text)\r\n",
        "#output_file.close()\r\n",
        "\r\n",
        "\r\n",
        "#print(\"The results of the stemming process are written to a file \" + output_file_name + \" and saved in the folder where this python file is located\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of the affix file: wb.xlsx\n",
            "Name of the text file: text4.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJLxzg9LuE2v"
      },
      "source": [
        "lines_seen = set() # holds lines already seen\r\n",
        "with open(\"stopwords.txt\", \"a+\") as stopwords_file:\r\n",
        "\tfor each_line in open(\"stems.txt\", \"r\"):\r\n",
        "\t    if each_line not in lines_seen: # check if line is not duplicate\r\n",
        "\t        stopwords_file.write(each_line)\r\n",
        "\t        lines_seen.add(each_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stpwag_JqU8h"
      },
      "source": [
        "with open(\"text4.txt\",  encoding=\"utf-8\") as f:\r\n",
        "    text_lines = f.readlines()\r\n",
        "\t\r\n",
        "with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    tmp_stoplines = f.readlines() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_NPAmMMqd7p"
      },
      "source": [
        "tmp_text_lines = []\r\n",
        "new_words = []\r\n",
        "stoplines = []\r\n",
        "for ii in tmp_stoplines:\r\n",
        "    if \"\\n\" in ii:\r\n",
        "        ii = ii.replace(\"\\n\", \"\")\r\n",
        "    stoplines.append(ii)\r\n",
        "j = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH8yrrGOA_IR"
      },
      "source": [
        "for line in text_lines:\r\n",
        "    tmp_words = \"\"\r\n",
        "    words = re.sub(r\"[,@\\?\\.$%_]\", \"\", line).split()\r\n",
        "    for word in words:\r\n",
        "        j = 0\r\n",
        "        found_in_stoplines = False\r\n",
        "        for z in range(len(word), 1, -1):\r\n",
        "            tubir=word[0:z]\r\n",
        "            affix=word[z:]\r\n",
        "            if tubir.lower() in stoplines:\r\n",
        "                found_in_stoplines = True\r\n",
        "                if affix in res:\r\n",
        "                    #index = endingsnew.index(affix)\r\n",
        "                    #affixs = res[affix]\r\n",
        "                    tmp_words += tubir + \"@@\" + res[affix] + \" \"\r\n",
        "                    tmp_text_lines.append(tmp_words)\r\n",
        "                    tmp_words = \"\"\r\n",
        "                    #print(\"aaa: \"+tubir+\" + \"+affix)\r\n",
        "                    j += 1\r\n",
        "                else:\r\n",
        "                    tmp_words += tubir + affix + \" \"\r\n",
        "                    tmp_text_lines.append(tmp_words)\r\n",
        "                    tmp_words = \"\"\r\n",
        "                    #print(\"bbb: \"+tubir+\" + \"+affix)\r\n",
        "                    j += 1\r\n",
        "                break\r\n",
        "            \r\n",
        "        if not found_in_stoplines:\r\n",
        "            if len(word) > 2:\r\n",
        "                tubir = \"\"\r\n",
        "                affix = \"\"\r\n",
        "                for i in range(2,len(word)):\r\n",
        "                    tubir=word[0:i]\r\n",
        "                    affix=word[i:]\r\n",
        "                    if affix in res:\r\n",
        "                        #index = endingsnew.index(affix)\r\n",
        "                        #affix = endingsnew_probel[index]\r\n",
        "                        tmp_words += tubir + \"@@\" + res[affix]\r\n",
        "                        tmp_text_lines.append(tmp_words)\r\n",
        "                        tmp_words = \"\"\r\n",
        "                        #print(\"eee: \"+tubir+\" + \"+affix)\r\n",
        "                        j += 1\r\n",
        "                    else:\r\n",
        "                        tmp_text_lines.append(word+\" \")\r\n",
        "                        #print(\"fff: \"+word)\r\n",
        "                        j += 1\r\n",
        "                    break\r\n",
        "                if j==0:\r\n",
        "                    tmp_text_lines.append(word+\" \")\r\n",
        "                    #print(\"hhh: \"+word)\r\n",
        "            else:\r\n",
        "                tmp_text_lines.append(word+\" \")\r\n",
        "                #print(\"ddd: \"+word)\r\n",
        "                \r\n",
        "    tmp_text_lines.append('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTNsoa50F9jx",
        "outputId": "c379d34c-73b4-452d-9140-c78e7c654d4a"
      },
      "source": [
        "tmp_text_lines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kiik ',\n",
              " 'iane ',\n",
              " \"shap@@g'an \",\n",
              " 'at@@lar ',\n",
              " 'ara@@sy@@nda ',\n",
              " 'qashyqtyq ',\n",
              " 'birte-birte ',\n",
              " 'qysqara ',\n",
              " \"tu's@@ti \",\n",
              " '\\n',\n",
              " 'Kedei ',\n",
              " \"auylyn' \",\n",
              " 'syrt@@y@@nda ',\n",
              " 'oina@@p ',\n",
              " 'iur@@gen ',\n",
              " 'top ',\n",
              " 'bala ',\n",
              " \"bu'l \",\n",
              " \"ko'rinis@@ke \",\n",
              " \"tan'irqa@@i \",\n",
              " 'qal@@yp@@ty ',\n",
              " 'Kiik@@ler ',\n",
              " \"ko'z@@den \",\n",
              " 'tasa ',\n",
              " 'bol@@yp ',\n",
              " \"A'sirese \",\n",
              " 'top ',\n",
              " 'ish@@i@@nde ',\n",
              " 'zhas@@y ',\n",
              " \"zhu'reg@@i@@nde \",\n",
              " \"a'lde \",\n",
              " 'bir ',\n",
              " \"ayanyshyn' \",\n",
              " 'iz@@i ',\n",
              " 'qal@@yp ',\n",
              " \"qoi@@g'an \",\n",
              " 'edi ',\n",
              " '\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLGfjPwZqnBZ"
      },
      "source": [
        "with open(\"segmented4.txt\", \"w\", encoding=\"utf-8\") as f:\r\n",
        "    for line in tmp_text_lines:\r\n",
        "        f.write('%s'%line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh4sex6L1bgj"
      },
      "source": [
        "def seg_accuracy(segmented_file, check_file):\r\n",
        "    segmented_file = open(segmented_file_name, 'r', encoding=\"utf-8\")\r\n",
        "    segmented_file = segmented_file.read()\r\n",
        "    check_file = open(check_file_name, 'r', encoding=\"utf-8\")\r\n",
        "    check_file = check_file.read()\r\n",
        "    segm= re.sub(r\"[,\\?\\.$%_]\", \"\", segmented_file).split()\r\n",
        "    check = re.sub(r\"[,\\?\\.$%_]\", \"\", check_file).split()\r\n",
        "    #[(x) for x in segm if x not in check] --incorrect words\r\n",
        "    incorrect=len([(x) for x in segm if x not in check])\r\n",
        "    no_samples=len(segm)\r\n",
        "    correct=no_samples-incorrect\r\n",
        "    accuracy= (correct/no_samples)*100\r\n",
        "    accuracies=[]\r\n",
        "    accuracies.append(accuracy)\r\n",
        "    with open(\"accuracies.txt\", \"a+\") as a:\r\n",
        "         for accuracy in accuracies:\r\n",
        "             a.write(str(accuracy)+ \"\\n\")     \r\n",
        "    return print(\"The accuracy of the segmented file is \"+ str(accuracy)+ \"%.\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3RydM_XU77L",
        "outputId": "d92276bf-7b94-47d4-ef86-d61a86543bcc"
      },
      "source": [
        "segmented_file_name=input('Name of the segmented file: ')\r\n",
        "check_file_name=input('Name of the file to compare: ')\r\n",
        "seg_accuracy(segmented_file_name, check_file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of the segmented file: segmented4.txt\n",
            "Name of the file to compare: check4.txt\n",
            "The accuracy of the segmented file is 94.44444444444444%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFFUOSIPwIg6",
        "outputId": "e7979f20-223e-49c5-e29b-af8116f25330"
      },
      "source": [
        "with open(\"accuracies.txt\", 'r') as a:\r\n",
        "     data = [float(line) for line in a]\r\n",
        "print(\"The average accuracy of segmentation is \", sum(data)/len(data),\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average accuracy of segmentation is  95.11708482676225 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}